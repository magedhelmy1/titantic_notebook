{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import re\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preparing Training Set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_train(dataframe):\n",
    "    # Drop the following columns\n",
    "    df_dropped = dataframe.drop(['PassengerId', 'Ticket'], axis=1)\n",
    "        \n",
    "    # Fix Names\n",
    "    df_dropped['Title'] = df_dropped.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\n",
    "    Title_Dictionary = {\n",
    "            \"Capt\":       \"Officer\",\n",
    "            \"Col\":        \"Officer\",\n",
    "            \"Major\":      \"Officer\",\n",
    "            \"Dr\":         \"Officer\",\n",
    "            \"Rev\":        \"Officer\",\n",
    "            \"Jonkheer\":   \"Royalty\",\n",
    "            \"Don\":        \"Royalty\",\n",
    "            \"Sir\" :       \"Royalty\",\n",
    "            \"the Countess\":\"Royalty\",\n",
    "            \"Dona\":       \"Royalty\",\n",
    "            \"Lady\" :      \"Royalty\",\n",
    "            \"Mme\":        \"Mrs\",\n",
    "            \"Ms\":         \"Mrs\",\n",
    "            \"Mrs\" :       \"Mrs\",\n",
    "            \"Mlle\":       \"Miss\",\n",
    "            \"Miss\" :      \"Miss\",\n",
    "            \"Mr\" :        \"Mr\",\n",
    "            \"Master\" :    \"Master\"\n",
    "                       }\n",
    "    \n",
    "    df_dropped['Title'] = df_dropped.Title.map(Title_Dictionary)\n",
    "\n",
    "\n",
    "    # Fix embarked\n",
    "    df_dropped[\"Embarked\"] = df_dropped[\"Embarked\"].fillna(value=\"C\")\n",
    "    \n",
    "    # Fix cabin\n",
    "    df_dropped['Cabin'] = df_dropped['Cabin'].fillna('U')\n",
    "    df_dropped['Cabin'] = df_dropped['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "\n",
    "\n",
    "\n",
    "    column = ['Pclass','Sex','Embarked','SibSp','Parch','Survived']\n",
    "    \n",
    "    for value in range(len(column), 0, -1):\n",
    "        df_dropped['Age'] = df_dropped['Age'].fillna(df_dropped.groupby(column[0:value])['Age'].transform('mean'))\n",
    "        df_dropped['Fare'] = df_dropped['Fare'].fillna(df_dropped.groupby(column[0:value])['Fare'].transform('mean'))\n",
    "\n",
    "\n",
    "    #if df_dropped.isnull().sum().sum() != 0:\n",
    "        #print(f\"Columns still has some null values {df_dropped.isnull().sum()}\")\n",
    "        \n",
    "    # Hot en-code male and female\n",
    "    df_dropped['Sex'] = df_dropped['Sex'].map({'female': 1, 'male': 0}).astype(int)\n",
    "    \n",
    "    # hot-encode cabins\n",
    "    cabin_one_hot = pd.get_dummies(df_dropped['Cabin'], prefix='Cabin')\n",
    "    df_dropped = df_dropped.drop('Cabin', axis=1)\n",
    "    df_dropped = df_dropped.join(cabin_one_hot)\n",
    "    \n",
    "    # Hot-encode names\n",
    "    name_one_hot = pd.get_dummies(df_dropped['Title'], prefix='Title')\n",
    "    df_dropped = df_dropped.drop('Title', axis=1)\n",
    "    df_dropped = df_dropped.drop('Name', axis=1)\n",
    "    df_dropped = df_dropped.join(name_one_hot)\n",
    "    \n",
    "\n",
    "    # Hot en-code Embarked\n",
    "    enbarked_one_hot = pd.get_dummies(df_dropped['Embarked'], prefix='Embarked')\n",
    "    df_dropped = df_dropped.drop('Embarked', axis=1)\n",
    "    df_dropped = df_dropped.join(enbarked_one_hot)\n",
    "    \n",
    "    \n",
    "    x = np.asarray(df_dropped.drop('Survived', axis=1))\n",
    "    scale = StandardScaler()\n",
    "    x_values = scale.fit_transform(x)\n",
    "\n",
    "    y_label = df_dropped[\"Survived\"].values\n",
    "    \n",
    "    return x_values, y_label, df_dropped.drop('Survived', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Preparing Test Set similiar to Training Set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_test(dataframe):\n",
    "    # Drop the following columns\n",
    "    df_dropped = dataframe.drop(['PassengerId', 'Ticket'], axis=1)\n",
    "        \n",
    "    # Fix Names\n",
    "    df_dropped['Title'] = df_dropped.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)\n",
    "    Title_Dictionary = {\n",
    "            \"Capt\":       \"Officer\",\n",
    "            \"Col\":        \"Officer\",\n",
    "            \"Major\":      \"Officer\",\n",
    "            \"Dr\":         \"Officer\",\n",
    "            \"Rev\":        \"Officer\",\n",
    "            \"Jonkheer\":   \"Royalty\",\n",
    "            \"Don\":        \"Royalty\",\n",
    "            \"Sir\" :       \"Royalty\",\n",
    "            \"the Countess\":\"Royalty\",\n",
    "            \"Dona\":       \"Royalty\",\n",
    "            \"Lady\" :      \"Royalty\",\n",
    "            \"Mme\":        \"Mrs\",\n",
    "            \"Ms\":         \"Mrs\",\n",
    "            \"Mrs\" :       \"Mrs\",\n",
    "            \"Mlle\":       \"Miss\",\n",
    "            \"Miss\" :      \"Miss\",\n",
    "            \"Mr\" :        \"Mr\",\n",
    "            \"Master\" :    \"Master\"\n",
    "                       }\n",
    "    \n",
    "    df_dropped['Title'] = df_dropped.Title.map(Title_Dictionary)\n",
    "\n",
    "\n",
    "    # Fix embarked\n",
    "    df_dropped[\"Embarked\"] = df_dropped[\"Embarked\"].fillna(value=\"C\")\n",
    "    \n",
    "    # Fix cabin\n",
    "    df_dropped['Cabin'] = df_dropped['Cabin'].fillna('U')\n",
    "    df_dropped['Cabin'] = df_dropped['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    df_dropped['Cabin_T'] = 0\n",
    "\n",
    "\n",
    "    column = ['Pclass','Sex','Embarked','SibSp','Parch']\n",
    "    \n",
    "    for value in range(len(column), 0, -1):\n",
    "        df_dropped['Age'] = df_dropped['Age'].fillna(df_dropped.groupby(column[0:value])['Age'].transform('mean'))\n",
    "        df_dropped['Fare'] = df_dropped['Fare'].fillna(df_dropped.groupby(column[0:value])['Fare'].transform('mean'))\n",
    "\n",
    "\n",
    "    #if df_dropped.isnull().sum().sum() != 0:\n",
    "        #print(f\"Columns still has some null values {df_dropped.isnull().sum()}\")\n",
    "        \n",
    "    # Hot en-code male and female\n",
    "    df_dropped['Sex'] = df_dropped['Sex'].map({'female': 1, 'male': 0}).astype(int)\n",
    "    \n",
    "    # hot-encode cabins\n",
    "    cabin_one_hot = pd.get_dummies(df_dropped['Cabin'], prefix='Cabin')\n",
    "    df_dropped = df_dropped.drop('Cabin', axis=1)\n",
    "    df_dropped = df_dropped.join(cabin_one_hot)\n",
    "    \n",
    "    # Hot-encode names\n",
    "    name_one_hot = pd.get_dummies(df_dropped['Title'], prefix='Title')\n",
    "    df_dropped = df_dropped.drop('Title', axis=1)\n",
    "    df_dropped = df_dropped.drop('Name', axis=1)\n",
    "    df_dropped = df_dropped.join(name_one_hot)\n",
    "    \n",
    "\n",
    "    # Hot en-code Embarked\n",
    "    enbarked_one_hot = pd.get_dummies(df_dropped['Embarked'], prefix='Embarked')\n",
    "    df_dropped = df_dropped.drop('Embarked', axis=1)\n",
    "    df_dropped = df_dropped.join(enbarked_one_hot)\n",
    "    \n",
    "    x = np.asarray(df_dropped)\n",
    "    scale = StandardScaler()\n",
    "    x_values = scale.fit_transform(x)\n",
    "    \n",
    "    return x_values, df_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Keras method </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train_1 = pd.read_csv(\"train.csv\")\n",
    "\n",
    "x_train, y_label, dataframe=data_preprocessing_train(df_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\envs\\tf_21\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\anaconda3\\envs\\tf_21\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 891 samples\n",
      "Epoch 1/100\n",
      "891/891 [==============================] - 0s 500us/sample - loss: 0.6948 - acc: 0.5802\n",
      "Epoch 2/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.6882 - acc: 0.6139\n",
      "Epoch 3/100\n",
      "891/891 [==============================] - 0s 81us/sample - loss: 0.6767 - acc: 0.6184\n",
      "Epoch 4/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.6673 - acc: 0.6229\n",
      "Epoch 5/100\n",
      "891/891 [==============================] - 0s 83us/sample - loss: 0.6559 - acc: 0.6240\n",
      "Epoch 6/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.6326 - acc: 0.6352\n",
      "Epoch 7/100\n",
      "891/891 [==============================] - 0s 79us/sample - loss: 0.6202 - acc: 0.6577\n",
      "Epoch 8/100\n",
      "891/891 [==============================] - 0s 81us/sample - loss: 0.6052 - acc: 0.6869\n",
      "Epoch 9/100\n",
      "891/891 [==============================] - 0s 82us/sample - loss: 0.5741 - acc: 0.6914\n",
      "Epoch 10/100\n",
      "891/891 [==============================] - 0s 83us/sample - loss: 0.5717 - acc: 0.7441\n",
      "Epoch 11/100\n",
      "891/891 [==============================] - 0s 87us/sample - loss: 0.5521 - acc: 0.7351\n",
      "Epoch 12/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.5522 - acc: 0.7430\n",
      "Epoch 13/100\n",
      "891/891 [==============================] - 0s 95us/sample - loss: 0.5438 - acc: 0.7329\n",
      "Epoch 14/100\n",
      "891/891 [==============================] - 0s 92us/sample - loss: 0.5531 - acc: 0.7407\n",
      "Epoch 15/100\n",
      "891/891 [==============================] - 0s 110us/sample - loss: 0.5393 - acc: 0.7419\n",
      "Epoch 16/100\n",
      "891/891 [==============================] - 0s 103us/sample - loss: 0.5309 - acc: 0.7587\n",
      "Epoch 17/100\n",
      "891/891 [==============================] - 0s 88us/sample - loss: 0.5224 - acc: 0.7688\n",
      "Epoch 18/100\n",
      "891/891 [==============================] - 0s 94us/sample - loss: 0.5636 - acc: 0.7396\n",
      "Epoch 19/100\n",
      "891/891 [==============================] - 0s 86us/sample - loss: 0.5150 - acc: 0.7632\n",
      "Epoch 20/100\n",
      "891/891 [==============================] - 0s 83us/sample - loss: 0.5222 - acc: 0.7733\n",
      "Epoch 21/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.4869 - acc: 0.7856\n",
      "Epoch 22/100\n",
      "891/891 [==============================] - 0s 82us/sample - loss: 0.4957 - acc: 0.7778\n",
      "Epoch 23/100\n",
      "891/891 [==============================] - 0s 87us/sample - loss: 0.5159 - acc: 0.7598\n",
      "Epoch 24/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.5179 - acc: 0.7744\n",
      "Epoch 25/100\n",
      "891/891 [==============================] - 0s 82us/sample - loss: 0.5146 - acc: 0.7643\n",
      "Epoch 26/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.4901 - acc: 0.7834\n",
      "Epoch 27/100\n",
      "891/891 [==============================] - 0s 94us/sample - loss: 0.5125 - acc: 0.7654\n",
      "Epoch 28/100\n",
      "891/891 [==============================] - 0s 88us/sample - loss: 0.4953 - acc: 0.7890\n",
      "Epoch 29/100\n",
      "891/891 [==============================] - 0s 92us/sample - loss: 0.5091 - acc: 0.7722\n",
      "Epoch 30/100\n",
      "891/891 [==============================] - 0s 81us/sample - loss: 0.5091 - acc: 0.7722\n",
      "Epoch 31/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.4951 - acc: 0.7789\n",
      "Epoch 32/100\n",
      "891/891 [==============================] - 0s 94us/sample - loss: 0.4833 - acc: 0.7856\n",
      "Epoch 33/100\n",
      "891/891 [==============================] - 0s 100us/sample - loss: 0.4866 - acc: 0.7733\n",
      "Epoch 34/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.4856 - acc: 0.7789\n",
      "Epoch 35/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.4916 - acc: 0.7856\n",
      "Epoch 36/100\n",
      "891/891 [==============================] - 0s 81us/sample - loss: 0.4833 - acc: 0.7834\n",
      "Epoch 37/100\n",
      "891/891 [==============================] - 0s 83us/sample - loss: 0.4870 - acc: 0.7755\n",
      "Epoch 38/100\n",
      "891/891 [==============================] - 0s 87us/sample - loss: 0.4712 - acc: 0.7901\n",
      "Epoch 39/100\n",
      "891/891 [==============================] - 0s 95us/sample - loss: 0.4904 - acc: 0.7935\n",
      "Epoch 40/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.5067 - acc: 0.7924\n",
      "Epoch 41/100\n",
      "891/891 [==============================] - 0s 100us/sample - loss: 0.4921 - acc: 0.7901\n",
      "Epoch 42/100\n",
      "891/891 [==============================] - 0s 97us/sample - loss: 0.4701 - acc: 0.8137\n",
      "Epoch 43/100\n",
      "891/891 [==============================] - 0s 88us/sample - loss: 0.4788 - acc: 0.7823\n",
      "Epoch 44/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.4629 - acc: 0.8013\n",
      "Epoch 45/100\n",
      "891/891 [==============================] - 0s 91us/sample - loss: 0.4387 - acc: 0.8272\n",
      "Epoch 46/100\n",
      "891/891 [==============================] - 0s 92us/sample - loss: 0.4690 - acc: 0.8148\n",
      "Epoch 47/100\n",
      "891/891 [==============================] - 0s 96us/sample - loss: 0.4624 - acc: 0.8159\n",
      "Epoch 48/100\n",
      "891/891 [==============================] - 0s 82us/sample - loss: 0.4487 - acc: 0.8238\n",
      "Epoch 49/100\n",
      "891/891 [==============================] - 0s 82us/sample - loss: 0.4464 - acc: 0.8137\n",
      "Epoch 50/100\n",
      "891/891 [==============================] - 0s 83us/sample - loss: 0.4560 - acc: 0.8148\n",
      "Epoch 51/100\n",
      "891/891 [==============================] - 0s 97us/sample - loss: 0.4628 - acc: 0.8272\n",
      "Epoch 52/100\n",
      "891/891 [==============================] - 0s 90us/sample - loss: 0.4534 - acc: 0.8070\n",
      "Epoch 53/100\n",
      "891/891 [==============================] - 0s 87us/sample - loss: 0.4589 - acc: 0.8025\n",
      "Epoch 54/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.4468 - acc: 0.8204\n",
      "Epoch 55/100\n",
      "891/891 [==============================] - 0s 86us/sample - loss: 0.4541 - acc: 0.8036\n",
      "Epoch 56/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.4782 - acc: 0.8081\n",
      "Epoch 57/100\n",
      "891/891 [==============================] - 0s 91us/sample - loss: 0.4614 - acc: 0.8070\n",
      "Epoch 58/100\n",
      "891/891 [==============================] - 0s 86us/sample - loss: 0.4356 - acc: 0.8272\n",
      "Epoch 59/100\n",
      "891/891 [==============================] - 0s 86us/sample - loss: 0.4445 - acc: 0.8171\n",
      "Epoch 60/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.4644 - acc: 0.7856\n",
      "Epoch 61/100\n",
      "891/891 [==============================] - 0s 82us/sample - loss: 0.4709 - acc: 0.8092\n",
      "Epoch 62/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.4447 - acc: 0.8182\n",
      "Epoch 63/100\n",
      "891/891 [==============================] - 0s 87us/sample - loss: 0.4264 - acc: 0.8328\n",
      "Epoch 64/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.4335 - acc: 0.8182\n",
      "Epoch 65/100\n",
      "891/891 [==============================] - 0s 79us/sample - loss: 0.4365 - acc: 0.8025\n",
      "Epoch 66/100\n",
      "891/891 [==============================] - 0s 90us/sample - loss: 0.4243 - acc: 0.8272\n",
      "Epoch 67/100\n",
      "891/891 [==============================] - 0s 99us/sample - loss: 0.4499 - acc: 0.8215\n",
      "Epoch 68/100\n",
      "891/891 [==============================] - 0s 96us/sample - loss: 0.4392 - acc: 0.8058\n",
      "Epoch 69/100\n",
      "891/891 [==============================] - 0s 90us/sample - loss: 0.4477 - acc: 0.8047\n",
      "Epoch 70/100\n",
      "891/891 [==============================] - 0s 90us/sample - loss: 0.4346 - acc: 0.8159\n",
      "Epoch 71/100\n",
      "891/891 [==============================] - 0s 83us/sample - loss: 0.4568 - acc: 0.8204\n",
      "Epoch 72/100\n",
      "891/891 [==============================] - 0s 79us/sample - loss: 0.4539 - acc: 0.7957\n",
      "Epoch 73/100\n",
      "891/891 [==============================] - 0s 83us/sample - loss: 0.4404 - acc: 0.8272\n",
      "Epoch 74/100\n",
      "891/891 [==============================] - 0s 91us/sample - loss: 0.4406 - acc: 0.8193\n",
      "Epoch 75/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.4538 - acc: 0.8126\n",
      "Epoch 76/100\n",
      "891/891 [==============================] - 0s 90us/sample - loss: 0.4497 - acc: 0.8159\n",
      "Epoch 77/100\n",
      "891/891 [==============================] - 0s 87us/sample - loss: 0.4432 - acc: 0.8204\n",
      "Epoch 78/100\n",
      "891/891 [==============================] - 0s 80us/sample - loss: 0.4461 - acc: 0.8137\n",
      "Epoch 79/100\n",
      "891/891 [==============================] - 0s 80us/sample - loss: 0.4107 - acc: 0.8361\n",
      "Epoch 80/100\n",
      "891/891 [==============================] - 0s 83us/sample - loss: 0.4481 - acc: 0.8260\n",
      "Epoch 81/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.4179 - acc: 0.8339\n",
      "Epoch 82/100\n",
      "891/891 [==============================] - 0s 111us/sample - loss: 0.4281 - acc: 0.8249\n",
      "Epoch 83/100\n",
      "891/891 [==============================] - 0s 94us/sample - loss: 0.4248 - acc: 0.8204\n",
      "Epoch 84/100\n",
      "891/891 [==============================] - 0s 88us/sample - loss: 0.4170 - acc: 0.8182\n",
      "Epoch 85/100\n",
      "891/891 [==============================] - 0s 92us/sample - loss: 0.4247 - acc: 0.8137\n",
      "Epoch 86/100\n",
      "891/891 [==============================] - 0s 81us/sample - loss: 0.4438 - acc: 0.8103\n",
      "Epoch 87/100\n",
      "891/891 [==============================] - 0s 79us/sample - loss: 0.4107 - acc: 0.8373\n",
      "Epoch 88/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.4425 - acc: 0.8126\n",
      "Epoch 89/100\n",
      "891/891 [==============================] - 0s 82us/sample - loss: 0.4175 - acc: 0.8260\n",
      "Epoch 90/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.4305 - acc: 0.8215\n",
      "Epoch 91/100\n",
      "891/891 [==============================] - 0s 86us/sample - loss: 0.4282 - acc: 0.8182\n",
      "Epoch 92/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.4097 - acc: 0.8440\n",
      "Epoch 93/100\n",
      "891/891 [==============================] - 0s 84us/sample - loss: 0.4447 - acc: 0.8182\n",
      "Epoch 94/100\n",
      "891/891 [==============================] - 0s 85us/sample - loss: 0.4109 - acc: 0.8440\n",
      "Epoch 95/100\n",
      "891/891 [==============================] - 0s 88us/sample - loss: 0.4137 - acc: 0.8193\n",
      "Epoch 96/100\n",
      "891/891 [==============================] - 0s 82us/sample - loss: 0.4447 - acc: 0.8272\n",
      "Epoch 97/100\n",
      "891/891 [==============================] - 0s 92us/sample - loss: 0.4236 - acc: 0.8272\n",
      "Epoch 98/100\n",
      "891/891 [==============================] - 0s 86us/sample - loss: 0.4288 - acc: 0.8159\n",
      "Epoch 99/100\n",
      "891/891 [==============================] - 0s 86us/sample - loss: 0.4445 - acc: 0.8171\n",
      "Epoch 100/100\n",
      "891/891 [==============================] - 0s 92us/sample - loss: 0.4132 - acc: 0.8238\n",
      "891/891 [==============================] - 0s 90us/sample - loss: 0.3531 - acc: 0.8620\n",
      "Model evaluation value is 0.8619528412818909\n"
     ]
    }
   ],
   "source": [
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(x_train.shape[1], activation=\"relu\",input_dim=x_train.shape[1]),\n",
    "    Dropout(0.20),\n",
    "    Dense(10, activation=\"relu\"),\n",
    "    Dropout(0.20),\n",
    "    Dense(8, activation=\"relu\"),\n",
    "    Dropout(0.20),\n",
    "    Dense(6, activation=\"relu\"),\n",
    "    Dropout(0.20),\n",
    "    Dense(3, activation=\"relu\"),\n",
    "    Dropout(0.20),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "model.fit(x_train, \n",
    "          y_label, \n",
    "          epochs=100,\n",
    "          callbacks=[tensorboard_callback])\n",
    "\n",
    "print(f\"Model evaluation value is {model.evaluate(x_train, y_label)[1]}\")\n",
    "\n",
    "# Model evaluation value is 0.868686854839325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418/418 [==============================] - 0s 45us/sample - loss: 0.5135 - acc: 0.7488\n",
      "Test Acc on actual results: 0.74880385\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"test_survi.csv\")\n",
    "y_test= df_test[\"Survived\"].values\n",
    "\n",
    "df_test_cleaned = df_test.drop('Survived', axis=1)\n",
    "x_test, dataframe_test=data_preprocessing_test(df_test_cleaned)\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print('Test Acc on actual results:', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If happy, execute this block and submit to kaggle\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "x_test=data_preprocessing_test(df_test)\n",
    "\n",
    "#fit with all values for submission:\n",
    "classes = model.predict_classes(x_test)\n",
    "df=pd.DataFrame(classes)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": df_test[\"PassengerId\"],\n",
    "        \"Survived\": df[0]\n",
    "    })\n",
    "submission.to_csv('titanic_keras.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Another Methods - Random Forest</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 7, 'max_features': 17}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "\n",
    "x_train, y_train, dataframe=data_preprocessing_train(df_train)\n",
    "\n",
    "# # Set our parameter grid\n",
    "# param_grid = {\n",
    "#     'max_features': [10,11,12,13,14,15,16,17,18,19,20,21,22,23],\n",
    "#     'max_depth': [3,5,7,9,11,13,15,16,17,18,19,20,21,22,23]\n",
    "\n",
    "# }\n",
    "# randomForest = RandomForestClassifier(random_state = 2)\n",
    "\n",
    "# randomForest_CV = GridSearchCV(estimator = randomForest, param_grid = param_grid, cv = 5)\n",
    "# randomForest_CV.fit(x_train, y_train)\n",
    "\n",
    "# randomForest_CV.best_params_\n",
    "\n",
    "#random_forest.score(x_train, y_train)\n",
    "\n",
    "# acc_random_forest = round(random_forest.score(x_train, y_train) * 100, 2)\n",
    "# print(round(acc_random_forest,2,), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=2, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForest = RandomForestClassifier(random_state = 2)\n",
    "\n",
    "randomForest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7703349282296651"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check with real data\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "df_test_survi = pd.read_csv(\"test_survi.csv\")\n",
    "df_test_cleaned = df_test_survi.drop('Survived', axis=1)\n",
    "true_label= df_test_survi[\"Survived\"].values\n",
    "\n",
    "X_test, dataframe =data_preprocessing_test(df_test_cleaned)\n",
    "\n",
    "y_pred = randomForest.predict(X_test)\n",
    "\n",
    "accuracy_score(true_label, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If happy, submit this to kaggle\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "X_test=data_preprocessing_test(df_test)\n",
    "\n",
    "\n",
    "y_pred = random_forest.predict(X_test)\n",
    "df=pd.DataFrame(y_pred)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": df_test[\"PassengerId\"],\n",
    "        \"Survived\": df[0]\n",
    "    })\n",
    "\n",
    "submission.to_csv('titanic_randomF.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
